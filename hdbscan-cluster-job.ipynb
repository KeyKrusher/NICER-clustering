{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b923fc19-d9b8-40e0-ae76-d5f21bbcb5da",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f82313-78d6-49a9-8e4b-e00213312e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f3ef63-3465-4663-8325-e5e58eb9f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pairwise_distances_chunked\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bcda9-bcc4-4fca-a130-8d23e19227af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/idies/workspace/Temporary/jaclar15/scratch/nicer/spectra2.csv')\n",
    "df = df.sort_values(by='TIME', ascending=True)\n",
    "df.TIME = df.TIME - df.TIME.iloc[0]\n",
    "df.TIME = pd.to_numeric(df.TIME, downcast='integer')\n",
    "df = df.set_index('TIME')\n",
    "df.columns = pd.to_numeric(df.columns, downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe6325-729e-4937-ba0c-b20d03252f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a training set (80%) and a testing set (20%)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71046d64-64a5-4955-aa30-4df32d86b450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to accumulate the distance matrix chunks\n",
    "def accumulate_distance_matrix(chunk, start, accumulated_matrix):\n",
    "    accumulated_matrix[start:start + chunk.shape[0], :] = chunk\n",
    "\n",
    "# Compute the pairwise Euclidean distances in chunks\n",
    "chunk_size = 1000\n",
    "n_chunks = int(np.ceil(train_df.shape[0] / chunk_size))\n",
    "\n",
    "# Create BallTree for faster distance calculation\n",
    "tree = BallTree(train_df, leaf_size=40)\n",
    "\n",
    "# Initialize an empty sparse distance matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "distance_matrix = lil_matrix((train_df.shape[0], train_df.shape[0]))\n",
    "\n",
    "# Compute the pairwise Euclidean distances in chunks\n",
    "for i in range(n_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, train_df.shape[0])\n",
    "    distances, _ = tree.query(train_df.iloc[start_idx:end_idx], k=train_df.shape[0]-1, return_distance=True)\n",
    "    accumulate_distance_matrix(distances, start_idx, distance_matrix)\n",
    "\n",
    "# Save the distance matrix to disk\n",
    "from scipy.sparse import save_npz\n",
    "save_npz('distance_matrix_sparse.npz', distance_matrix.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251a3cb-1fcf-4494-81e7-a7e0997f23e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the distance matrix from disk\n",
    "# from scipy.sparse import load_npz\n",
    "# loaded_distance_matrix = load_npz('distance_matrix_sparse.npz')\n",
    "\n",
    "# # Make clusters\n",
    "# clusterer = hdbscan.HDBSCAN(metric='precomputed', min_samples=5, core_dist_n_jobs=-2)\n",
    "# # Dump to a pickle file\n",
    "# dump(clusterer, open('untrained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# clusterer.fit(loaded_distance_matrix)\n",
    "# dump(clusterer, open('trained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# y_pred = clusterer.labels_\n",
    "# y_prob = clusterer.probabilities_\n",
    "\n",
    "# # Plot clusters\n",
    "# # plt = df.groupby(y_pred).agg('mean').T.plot()\n",
    "# # Add the cluster labels to the DataFrame\n",
    "# df_with_labels = df.assign(cluster_labels=y_pred)\n",
    "\n",
    "# # Plot clusters\n",
    "# plt = df_with_labels.groupby('cluster_labels').agg('mean').T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d304bf-f26b-4cbf-9489-4b57af09c592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Make clusters\n",
    "# clusterer = hdbscan.HDBSCAN(metric='euclidean', min_samples=5, core_dist_n_jobs=-2)\n",
    "# # dump to a pickle file\n",
    "# dump(clusterer, open('untrained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# clusterer.fit(train_df)\n",
    "# dump(clusterer, open('trained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# y_pred = clusterer.labels_\n",
    "# y_prob = clusterer.probabilities_ # The hdbscan library implements soft clustering, where each data point is assigned a cluster \n",
    "#                                   # membership score ranging from 0.0 to 1.0. A score of 0.0 represents a sample that is not in the cluster at all\n",
    "# # Plot clusters\n",
    "# plt = df.groupby(y_pred).agg('mean').T.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3ac29bc-7055-493d-933f-6f3f1053df02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52de26-d2f0-4939-94a7-50199a589085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_df = df.sample(n=1000)\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN(metric='euclidean', min_samples=5)\n",
    "# dump(clusterer, open('untrained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# clusterer.fit(test_df)\n",
    "# dump(clusterer, open('trained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "# y_pred = clusterer.labels_\n",
    "# # y_prob = clusterer.probabilities_ # Probability \n",
    "# # Plot clusters\n",
    "# cluster_std = test_df.groupby(y_pred).agg(np.std)\n",
    "# plt = test_df.groupby(y_pred).agg('mean').T.plot(yerr=cluster_std.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1bfd48d-47d3-45aa-ae28-7875a0a80559",
   "metadata": {},
   "source": [
    "### Testing BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf33e77-e796-4bac-832c-b6d38384889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(n=10_000)\n",
    "# Function to accumulate the distance matrix chunks\n",
    "def accumulate_distance_matrix(chunk, start, accumulated_matrix):\n",
    "    accumulated_matrix[start:start + chunk.shape[0], :] = chunk\n",
    "\n",
    "# Compute the pairwise Euclidean distances in chunks\n",
    "chunk_size = 1000\n",
    "n_chunks = int(np.ceil(train_df.shape[0] / chunk_size))\n",
    "\n",
    "# Create BallTree for faster distance calculation\n",
    "tree = BallTree(train_df, leaf_size=40)\n",
    "\n",
    "# Initialize an empty sparse distance matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "distance_matrix = lil_matrix((train_df.shape[0], train_df.shape[0]))\n",
    "\n",
    "# Compute the pairwise Euclidean distances in chunks\n",
    "for i in range(n_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, train_df.shape[0])\n",
    "    distances, _ = tree.query(train_df.iloc[start_idx:end_idx], k=train_df.shape[0], return_distance=True)\n",
    "    accumulate_distance_matrix(distances, start_idx, distance_matrix)\n",
    "\n",
    "# Save the distance matrix to disk\n",
    "from scipy.sparse import save_npz\n",
    "save_npz('distance_matrix_sparse.npz', distance_matrix.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3a26c-6777-4972-aae0-8c2777737ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the distance matrix from disk\n",
    "from scipy.sparse import load_npz\n",
    "loaded_distance_matrix = load_npz('distance_matrix_sparse.npz')\n",
    "\n",
    "# Make clusters\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', min_samples=5, core_dist_n_jobs=-2)\n",
    "# Dump to a pickle file\n",
    "dump(clusterer, open('untrained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "clusterer.fit(loaded_distance_matrix)\n",
    "dump(clusterer, open('trained-hdbscan_cluster-job.joblib', 'wb'))\n",
    "y_pred = clusterer.labels_\n",
    "y_prob = clusterer.probabilities_\n",
    "\n",
    "# Plot clusters\n",
    "# plt = df.groupby(y_pred).agg('mean').T.plot()\n",
    "# Add the cluster labels to the DataFrame\n",
    "# df_with_labels = test_df.assign(cluster_labels=y_pred)\n",
    "\n",
    "# # Plot clusters\n",
    "# plt = df_with_labels.groupby('cluster_labels').agg('mean').T.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(heasoft)",
   "language": "python",
   "name": "heasoft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
